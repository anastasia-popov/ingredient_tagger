{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ingredient Tagger: model selection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtD9blvTR6DP",
        "outputId": "c9ca6fd6-07a4-4f04-93b4-c20ecf224754"
      },
      "source": [
        "# Install all the necessary packages\n",
        "!pip install pytorch-crf requests regex sentencepiece sacremoses tokenizers transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.6/dist-packages (0.7.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.95)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (0.0.43)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.3.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZfiLtbjTQwA"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchcrf import CRF\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW,  BertConfig, BertForTokenClassification, get_linear_schedule_with_warmup\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import urllib.request\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc_I_PDBUFqG",
        "outputId": "0654de99-059d-4f3d-bce8-744914049c6f"
      },
      "source": [
        "# Download the data\n",
        "urllib.request.urlretrieve('https://ingredient-tagger.s3-us-west-2.amazonaws.com/train_file_cleaned.csv', 'train.csv')\n",
        "urllib.request.urlretrieve('https://ingredient-tagger.s3-us-west-2.amazonaws.com/test_file_cleaned.csv', 'test.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('test.csv', <http.client.HTTPMessage at 0x7f2a9fb461d0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfriMbNTUvht"
      },
      "source": [
        "# Read the data to train and test pandas dataframes\n",
        "train_df = pd.read_csv('train.csv', delimiter='\\t', header=None,\n",
        "                       usecols=[0,1,5],  names=['word', 'wordnum', 'label'])\n",
        "test_df = pd.read_csv('test.csv', delimiter='\\t', header=None,\n",
        "                      usecols=[0,1,5],  names=['word', 'wordnum', 'label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5kQi8o-VCuG"
      },
      "source": [
        "# Functions for processing data\n",
        "\n",
        "MAX_LEN = 30\n",
        "\n",
        "def build_word_dictionary(df):\n",
        "    \"\"\"\n",
        "    Builds a dictionary of word:token_id from the training dataframe df\n",
        "    \"\"\"\n",
        "    words = list(set(df.word.values))\n",
        "    word_dict = {w: i+1 for i, w in enumerate(words)}\n",
        "    word_dict['UKN'] = len(word_dict)\n",
        "    word_dict['PAD'] = 0\n",
        "    return word_dict\n",
        "\n",
        "\n",
        "def data_to_sentences(df):\n",
        "    \"\"\"\n",
        "    Extracts sentences and tokens out of the dataframe df\n",
        "\n",
        "    Parameters:\n",
        "        df  - pandas dataframe with the data\n",
        "\n",
        "    Returns: \n",
        "        sentence_list - list of sentences, split to list of words\n",
        "        labels_list - list of label lists for each sentence\n",
        "    \"\"\"\n",
        "    sentence = []\n",
        "    sentence_list = []\n",
        "    labels = []\n",
        "    labels_list = []\n",
        "    for row in df.itertuples():   \n",
        "        if row.wordnum == 'I1':\n",
        "            sentence_list.append(sentence)\n",
        "            labels_list.append(labels)\n",
        "            sentence = []\n",
        "            labels = []\n",
        "        sentence.append(row.word)\n",
        "        labels.append(row.label)\n",
        "\n",
        "    sentence_list.append(sentence)\n",
        "    labels_list.append(labels)\n",
        "    \n",
        "    sentence_list.pop(0)\n",
        "    labels_list.pop(0)\n",
        "    \n",
        "    return sentence_list, labels_list\n",
        "\n",
        "\n",
        "def tokenize_and_pad_sentences(sentence_list, tokenizer, maxlen):\n",
        "    \"\"\"\n",
        "    Tokenizes the list of sentences, and pads them to the maxlen length\n",
        "    \n",
        "    Parameters:\n",
        "        sentence_list (list) - list of sentences, split by words\n",
        "        tokenizer (Tokenizer) - tokenizer\n",
        "        maxlen (int) - lengh to pad/truncate the sentences to\n",
        "\n",
        "    Returns:\n",
        "        padded_sents (list) - tokenized and padded sentences\n",
        "    \"\"\"\n",
        "    sentences = [\" \".join([word for word in sent]) for sent in sentence_list]\n",
        "    encoded_sents = [tokenizer.encode(sent,add_special_tokens = True) \n",
        "                     for sent in sentences]\n",
        "    padded_sents = pad_sequences(encoded_sents, maxlen=maxlen, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "    return padded_sents\n",
        "\n",
        "\n",
        "def convert_and_pad_labels(tags_list, tag_dict, maxlen):\n",
        "    \"\"\"\n",
        "    Converts the list of tag sequences to integers and pads them to the maxlen\n",
        "    length\n",
        "\n",
        "    Parameters:\n",
        "        tag_list (list) - list of tag sequences\n",
        "        tag_dict (dict) - dictionary of tags to their integer ids\n",
        "        maxlen (int) - length to pad/truncate the tag sequences to\n",
        "\n",
        "    Returns:\n",
        "        padded_tags (list) - padded sequences of tag ids\n",
        "    \"\"\"\n",
        "    tag_ids = [[tag_dict[t] for t in s] for s in tags_list]\n",
        "    padded_tags = pad_sequences(tag_ids, maxlen=maxlen, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "    return padded_tags\n",
        "\n",
        "def convert_and_pad_sentences(sentence_list, word_dict, maxlen):\n",
        "    \"\"\"\n",
        "    Converts the list of sentences to lists of word integer ids and pads \n",
        "    them to the maxlen length\n",
        "\n",
        "    Parameters:\n",
        "        sentence_list (list) - list of sentences split by words\n",
        "        word_dict (dict) - dictionary of words to their integer ids\n",
        "        maxlen (int) - length to pad/truncate the sentences to\n",
        "\n",
        "    Returns:\n",
        "        padded_words (list) - padded sequences of word ids\n",
        "    \"\"\"\n",
        "    word_ids = [[word_dict.get(w, word_dict['UKN']) for w in s] \n",
        "                for s in sentence_list]\n",
        "    padded_words = pad_sequences(word_ids, maxlen=maxlen, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "    return padded_words\n",
        "\n",
        "\n",
        "def process_data(df, tokenizer, tag_dict, word_dict, maxlen):\n",
        "    \"\"\"\n",
        "    Converts the data from dataframe df to the paded sequences of integer ids\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame) - dataframe with the data\n",
        "        tokenizer (Tokenizer) - tokenizer for word sequences\n",
        "        tag_dict (dict) - dictionary of tags and their integer ids\n",
        "        word_dict (dict) - dictionary of words and theri integer ids\n",
        "        maxlen (int) - length to pad/trankate sequences to\n",
        "\n",
        "    Returns:\n",
        "        X_bert - tokenized and padded list of sentences (BertTokenizer)\n",
        "        X - list of sentences, padded and converted to list of word integer ids\n",
        "        y = list of tags, padded and converted to their integer ids   \n",
        "    \"\"\"\n",
        "    sentences, labels = data_to_sentences(df)\n",
        "    X_bert = tokenize_and_pad_sentences(sentences, tokenizer, maxlen)\n",
        "    X = convert_and_pad_sentences(sentences, word_dict, maxlen)\n",
        "    y = convert_and_pad_labels(labels, tag_dict, maxlen)\n",
        "\n",
        "    return X_bert, X, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywPuEWUvEoFp"
      },
      "source": [
        "tag_dict = {'PAD': 0, \n",
        "            'B-QTY': 1,  \n",
        "            'B-RANGE_END': 2,\n",
        "            'B-UNIT': 3,  \n",
        "            'I-UNIT': 4,\n",
        "            'B-NAME': 5,\n",
        "            'I-NAME': 6,\n",
        "            'OTHER': 7}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48T0G2JEqFK1"
      },
      "source": [
        "word_dict = build_word_dictionary(train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9XjFJ6brs6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4274bf0b-befd-4a10-e11c-4b18614f3f5c"
      },
      "source": [
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', \n",
        "                           'tokenizer', \n",
        "                           'bert-base-uncased')   \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2g0D0cLqNPk"
      },
      "source": [
        "train_X_bert_full, train_X_full, train_y_full = process_data(train_df, tokenizer, tag_dict, word_dict, MAX_LEN)\n",
        "test_X_bert, test_X, test_y = process_data(test_df, tokenizer, tag_dict, word_dict, MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okNkCnHZxU9C"
      },
      "source": [
        "def get_attention_masks(input_ids):\n",
        "    \"\"\"\n",
        "    Returns list of attention masks for ids greater than pad id\n",
        "    \"\"\"\n",
        "    return [[int(token_id > 0) for token_id in sent] for sent in input_ids]\n",
        "\n",
        "\n",
        "def get_data_loader(batch_size, train_x, train_y):\n",
        "    \"\"\"\n",
        "    Builds pytorch dataloader\n",
        "    \"\"\"\n",
        "\n",
        "    attention_masks = get_attention_masks(train_x)\n",
        "        \n",
        "    train_ds = TensorDataset(torch.tensor(train_x), \n",
        "                             torch.ByteTensor(attention_masks), \n",
        "                             torch.tensor(train_y))\n",
        "    train_sampler = RandomSampler(train_ds)\n",
        "\n",
        "\n",
        "    return torch.utils.data.DataLoader(train_ds,\n",
        "                                       sampler=train_sampler,\n",
        "                                       batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z75AFBnGxaam"
      },
      "source": [
        "train_X, val_X, _, _ = train_test_split(train_X_full, train_y_full,  random_state=1, test_size=0.1)     \n",
        "train_X_bert, val_X_bert, train_y, val_y = train_test_split(train_X_bert_full, train_y_full,  random_state=1, test_size=0.1)                                     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe2ObGTpyDUM"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataloader_full = get_data_loader(batch_size, train_X_full, train_y_full)\n",
        "train_dataloader_bert_full = get_data_loader(batch_size, train_X_bert_full, train_y_full)\n",
        "\n",
        "train_dataloader = get_data_loader(batch_size, train_X, train_y)\n",
        "train_dataloader_bert = get_data_loader(batch_size, train_X_bert, train_y)\n",
        "\n",
        "val_dataloader = get_data_loader(batch_size, val_X, val_y)\n",
        "val_dataloader_bert = get_data_loader(batch_size, val_X_bert, val_y)\n",
        "\n",
        "test_dataloader = get_data_loader(batch_size, test_X, test_y)\n",
        "test_dataloader_bert = get_data_loader(batch_size, test_X_bert, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zII0vjLIEGf"
      },
      "source": [
        "class Identity(nn.Module):\n",
        "    \"\"\"\n",
        "    Auxilary Identiy layer to remove the last Bert layer when we need to add\n",
        "    other layers on top\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "    \n",
        "class IngredientTagger(nn.Module):\n",
        "    \"\"\"\n",
        "    Main model for ingredient tagging\n",
        "    \"\"\"\n",
        "    def __init__(self, num_tags, use_bert, use_rnn, vocab_size, word_embed_dim, rnn_hidden_dim, dropout, use_crf):\n",
        "        \"\"\"\n",
        "        Model initialization\n",
        "\n",
        "        Parameters:\n",
        "            num_tags (int) - total number of tags\n",
        "            use_bert (bool) - use Bert layer if True\n",
        "            use_rnn (bool) - use LSTM layer if True\n",
        "            vocab_size (int) - total number of words in the vocabulary\n",
        "            word_embed_dim (int) - dimention of the Embedding layer\n",
        "            rnn_hidden_dim (int)- dimention of the LSTM hidden states\n",
        "            use_crf (bool) - use CRF layer if True\n",
        "\n",
        "        \"\"\"\n",
        "        super(IngredientTagger, self).__init__()\n",
        "       \n",
        "        self.use_bert = use_bert\n",
        "        if self.use_bert:\n",
        "            self.bert = BertForTokenClassification.from_pretrained(   \n",
        "                \"bert-base-uncased\", \n",
        "                num_labels = num_tags, \n",
        "                output_attentions = False, \n",
        "                output_hidden_states = False,\n",
        "            )\n",
        "          \n",
        "            enc_size = 768\n",
        "        else:\n",
        "           self.input_dim = vocab_size\n",
        "           self.embeds = nn.Embedding(vocab_size, word_embed_dim, padding_idx=0)\n",
        "           enc_size = word_embed_dim\n",
        "\n",
        "        self.use_rnn=use_rnn\n",
        "        if self.use_rnn:\n",
        "            if self.use_bert:\n",
        "                self.bert.classifier = Identity() \n",
        "            self.rnn = nn.LSTM(bidirectional=True,\n",
        "                               input_size=enc_size,\n",
        "                               hidden_size=rnn_hidden_dim//2,\n",
        "                               batch_first=True,\n",
        "                               dropout=dropout)  \n",
        "            self.drop = nn.Dropout(dropout)\n",
        "            enc_size = rnn_hidden_dim \n",
        "\n",
        "            \n",
        "        \n",
        "        self.fc1 = nn.Linear(enc_size, num_tags)\n",
        "\n",
        "        self.use_crf = use_crf \n",
        "        if self.use_crf:\n",
        "            self.crf = CRF(num_tags, batch_first=True)\n",
        "        \n",
        "\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    \n",
        "    def __str__(self):\n",
        "      \"\"\"\n",
        "      Returns the string representaiton of the model\n",
        "      \"\"\"\n",
        "      return f'BERT: {self.use_bert}, LSTM: {self.use_rnn}, CRF: {self.use_crf}'\n",
        "   \n",
        "    def loss(self, x, y, attention_mask):\n",
        "        \"\"\"\n",
        "        Return the loss after one forward pass\n",
        "\n",
        "        Parameters:\n",
        "            x (LongTensor) - tensor of tokenized and padded sentences\n",
        "            y (LongTensor) - tensor of tokenized and padded tags\n",
        "            attention_mask(ByteTensor) - tensor of attention_masks\n",
        "\n",
        "        Returns:\n",
        "            loss - loss after one model forward pass  \n",
        "\n",
        "        \"\"\"\n",
        "           \n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        if self.use_bert:\n",
        "            if self.use_rnn:\n",
        "                encoded_layers = self.bert(x,\n",
        "                                           attention_mask=attention_mask,\n",
        "                                           token_type_ids=None)\n",
        "            else:\n",
        "                encoded_layers = self.bert(x,\n",
        "                                           labels=y,\n",
        "                                           attention_mask=attention_mask,\n",
        "                                           token_type_ids=None)\n",
        "            bert_loss  = encoded_layers.loss\n",
        "            logits = encoded_layers.logits\n",
        "        else:\n",
        "            logits = self.embeds(x)\n",
        "            \n",
        "        if self.use_rnn:\n",
        "            enc, _ = self.rnn(logits)  \n",
        "            enc = self.drop(enc)\n",
        "            logits = self.fc1(enc) \n",
        "            \n",
        "        if self.use_crf:\n",
        "                loss = -self.crf(logits, y, attention_mask)\n",
        "        elif self.use_rnn:\n",
        "                loss = nn.CrossEntropyLoss()(logits.view(-1, logits.shape[-1]),y.view(-1))\n",
        "        else:\n",
        "                loss = bert_loss\n",
        "      \n",
        "        return loss\n",
        "        \n",
        "        \n",
        "    def forward(self, x, attention_mask):\n",
        "        \"\"\"\n",
        "        Model's forward pass (used for prediction)\n",
        "        \"\"\"\n",
        "\n",
        "        x = x.to(self.device) \n",
        "       \n",
        "        with torch.no_grad():\n",
        "            if self.use_bert:\n",
        "                self.bert.eval()\n",
        "                encoded_layers = self.bert(x,\n",
        "                                          attention_mask=attention_mask,\n",
        "                                          token_type_ids=None) \n",
        "                logits = encoded_layers.logits  \n",
        "            else:\n",
        "                logits = self.embeds(x)\n",
        "\n",
        "            if self.use_rnn:\n",
        "                enc, _ = self.rnn(logits)  \n",
        "                logits = self.fc1(enc)  \n",
        " \n",
        "            if self.use_crf:\n",
        "                y_hat =  torch.tensor(self.crf.decode(logits))\n",
        "            else:\n",
        "                y_hat = logits.argmax(-1) \n",
        "    \n",
        "        return y_hat\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bD7iR5qy8KU",
        "outputId": "383dfb73-ae67-4d0f-d542-3d0794b38149"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUtMGst0DGsu"
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    \"\"\"\n",
        "    Computes accuracy given arrays of predictions and labels\n",
        "\n",
        "    Parameters:\n",
        "        preds (np.ndarray) - array of predictions\n",
        "        labels (np.ndarray) - array of ground truth labels\n",
        "    \"\"\"\n",
        "    pred_flat = preds.flatten() \n",
        "    labels_flat = labels.flatten()\n",
        "    mask = labels_flat != 0\n",
        "    return np.sum(pred_flat[mask] == labels_flat[mask]) / len(labels_flat[mask])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAD94FcwNLSP"
      },
      "source": [
        "def evaluate(model, dataloader):\n",
        "    \"\"\"\n",
        "    Returns accuracy of the model on validation/test data\n",
        "    \"\"\"\n",
        "    \n",
        "    model.eval()\n",
        "  \n",
        "    test_preds = []\n",
        "    test_y = []\n",
        "\n",
        " \n",
        "    for batch in dataloader:\n",
        "        \n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        input_ids, input_mask, labels = batch\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(input_ids, \n",
        "                            attention_mask=input_mask)\n",
        "\n",
        "        preds = outputs\n",
        "    \n",
        "        logits =  preds.to('cpu').numpy() \n",
        "        label_ids = labels.to('cpu').numpy()\n",
        "         \n",
        "        test_preds.append(logits)\n",
        "        test_y.append(label_ids)\n",
        "        \n",
        "    correct = 0\n",
        "    n = 0\n",
        "\n",
        "    for pred, label in zip(test_preds, test_y):\n",
        "        mask = label != 0\n",
        "        correct += (pred[mask] == label[mask]).sum()\n",
        "        n += len(label[mask])\n",
        "      \n",
        "    accuracy = correct/n\n",
        "   \n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFGXGvphY9dx"
      },
      "source": [
        "def format_time(elapsed):\n",
        "    \"\"\"\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    \"\"\"\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDZKOpwRyzoC"
      },
      "source": [
        "def train_model(model, train_dataloader, validation_dataloader, epochs, optimizer, scheduler):\n",
        "    \"\"\"\n",
        "    Trains and the model, using early stopping\n",
        "\n",
        "    Parameter:\n",
        "        model (IngredientTagger) - model to train\n",
        "        train_dataloader (Dataloader) - dataloader for train dataset\n",
        "        validation_dataloader (Dataloader) - dataloader for validation dataset\n",
        "        epochs (int) - maximum number of epochs to train\n",
        "        optimizer (Optimizer) - optimizer used for training\n",
        "        scheduler (Scheduler) - scheduler for the learning rate \n",
        "\n",
        "    Returns:\n",
        "       model - trained model\n",
        "\n",
        "    \"\"\"\n",
        "    seed_val = 1\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "   \n",
        "    model.to(device)\n",
        "    t0 = time.time()\n",
        "\n",
        "    best_acc = 0\n",
        "    early_stop = 0 \n",
        "    patience = 2\n",
        "    model_name = 'best_model'\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "       \n",
        "        total_loss = 0\n",
        "      \n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            \n",
        "            input_ids = batch[0].to(device)\n",
        "            input_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            model.zero_grad()        \n",
        "\n",
        "            loss = model.loss(input_ids, labels, input_mask)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0.\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "        avg_train_loss = total_loss / len(train_dataloader)  \n",
        "\n",
        "        # Validation\n",
        "        acc = evaluate(model, validation_dataloader)\n",
        "        if acc > best_acc:\n",
        "           best_acc = acc\n",
        "           early_stop = 0\n",
        "           torch.save(model.state_dict(), model_name)\n",
        "           best_epoch = epoch\n",
        "        else:\n",
        "           early_stop += 1\n",
        "\n",
        "        if early_stop > patience:\n",
        "          print(f'Epoch {best_epoch}: early stop')\n",
        "          print(f'Accuracy on validation dataset: {best_acc}')\n",
        "          break\n",
        "\n",
        "    model.load_state_dict(torch.load(model_name))\n",
        " \n",
        "    print(f'Training took: {format_time(time.time() - t0)}')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeBJs2AoCRHw"
      },
      "source": [
        "# List of parameters for Embedding, LSTM, CRF and fc layers\n",
        "non_bert_params=['embeds.weight',\n",
        "                'rnn.weight_ih_l0',\n",
        "                'rnn.weight_hh_l0',\n",
        "                'rnn.bias_ih_l0',\n",
        "                'rnn.bias_hh_l0',\n",
        "                'rnn.weight_ih_l0_reverse',\n",
        "                'rnn.weight_hh_l0_reverse',\n",
        "                'rnn.bias_ih_l0_reverse',\n",
        "                'rnn.bias_hh_l0_reverse',\n",
        "                'fc1.weight',\n",
        "                'fc1.bias',\n",
        "                'crf.start_transitions',\n",
        "                'crf.end_transitions',\n",
        "                'crf.transitions']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOopE-GyBlJ2"
      },
      "source": [
        "def freeze_bert(model):\n",
        "  print('Freezing BERT layers')\n",
        "  for param in model.named_parameters():\n",
        "    if param[0] not in non_bert_params:\n",
        "      param[1].requires_grad = False\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jxu4LPPBmJS"
      },
      "source": [
        "def unfreeze(model):\n",
        "  print('Unfreezing BERT for fine tuning')\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCQfAQE5KbDn"
      },
      "source": [
        "def train_and_evaluate(model, epochs):\n",
        "  \"\"\"\n",
        "  Trains the model, using early stopping, and evaluates it on the test dataset\n",
        "\n",
        "  Parameters:\n",
        "      model (IngredientTagger) - model to train and evaluate\n",
        "      epochs (int) - maximum number of epochs to train the model\n",
        "\n",
        "  Returns:\n",
        "     accuracy - accuracy of the model on the test dataset\n",
        "  \"\"\"\n",
        " \n",
        "  # We'll use different learning rate for BERT and subsequent layers\n",
        "  params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] in non_bert_params, model.named_parameters()))))\n",
        "  base_params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] not in non_bert_params, model.named_parameters()))))\n",
        "\n",
        "  optimizer = AdamW([{'params': base_params}, {'params': params, 'lr': 1e-4}], lr=2e-5)\n",
        "\n",
        "  if model.use_bert:\n",
        "    train_data_loader = train_dataloader_bert\n",
        "    test_data_loader = test_dataloader_bert\n",
        "    val_data_loader = val_dataloader_bert\n",
        "  else:\n",
        "    train_data_loader = train_dataloader\n",
        "    test_data_loader = test_dataloader\n",
        "    val_data_loader = val_dataloader\n",
        "\n",
        "  total_steps = len(train_data_loader) * epochs\n",
        "\n",
        "  num_warmup_steps = 0\n",
        "  num_training_steps = total_steps\n",
        "\n",
        "  # Lambda for warmup scheduler for BERT learning rate\n",
        "  def lr_lambda(current_step: int):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(\n",
        "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        )\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
        "                                                lr_lambda=[lr_lambda,\n",
        "                                                           lambda epoch: 0.98 ** epoch])\n",
        "  if model.use_bert and (model.use_rnn or model.use_crf):\n",
        "    model = freeze_bert(model)\n",
        "    model = train_model(model, train_data_loader, val_data_loader, epochs, optimizer, scheduler)\n",
        "    model = unfreeze(model)\n",
        "  model = train_model(model, train_data_loader, val_data_loader, epochs, optimizer, scheduler)\n",
        "  accuracy = evaluate(model, test_data_loader)\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1biOgZQJKFsM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce9c877a-c981-4a96-c5d1-321bdc053e76"
      },
      "source": [
        "# # Initiate the list of model configurations to test\n",
        "word_embed_dim = 300\n",
        "rnn_hidden_dim = 200\n",
        "dropout = 0.5\n",
        "\n",
        "models = [IngredientTagger(len(tag_dict),\n",
        "                           use_bert=False, \n",
        "                           use_rnn=True, \n",
        "                           vocab_size=len(word_dict),\n",
        "                           word_embed_dim=word_embed_dim,\n",
        "                           rnn_hidden_dim=rnn_hidden_dim,\n",
        "                           dropout = dropout,\n",
        "                           use_crf=False),\n",
        "          IngredientTagger(len(tag_dict),\n",
        "                           use_bert=False, \n",
        "                           use_rnn=True, \n",
        "                           vocab_size=len(word_dict),\n",
        "                           word_embed_dim=word_embed_dim,\n",
        "                           rnn_hidden_dim=rnn_hidden_dim,\n",
        "                           dropout = dropout,\n",
        "                           use_crf=True),\n",
        "          IngredientTagger(len(tag_dict),\n",
        "                           use_bert=True, \n",
        "                           use_rnn=False, \n",
        "                           vocab_size=len(word_dict),\n",
        "                           word_embed_dim=word_embed_dim,\n",
        "                           rnn_hidden_dim=rnn_hidden_dim,\n",
        "                           dropout = dropout, \n",
        "                           use_crf=False),\n",
        "          IngredientTagger(len(tag_dict),\n",
        "                           use_bert=True, \n",
        "                           use_rnn=True, \n",
        "                           vocab_size=len(word_dict),\n",
        "                           word_embed_dim=word_embed_dim,\n",
        "                           rnn_hidden_dim=rnn_hidden_dim,\n",
        "                           dropout = dropout, \n",
        "                           use_crf=False),\n",
        "          IngredientTagger(len(tag_dict),\n",
        "                           use_bert=True, \n",
        "                           use_rnn=False, \n",
        "                           vocab_size=len(word_dict),\n",
        "                           word_embed_dim=word_embed_dim,\n",
        "                           rnn_hidden_dim=rnn_hidden_dim,\n",
        "                           dropout = dropout, \n",
        "                           use_crf=True),\n",
        "          IngredientTagger(len(tag_dict),\n",
        "                           use_bert=True, \n",
        "                           use_rnn=True, \n",
        "                           vocab_size=len(word_dict),\n",
        "                           word_embed_dim=word_embed_dim,\n",
        "                           rnn_hidden_dim=rnn_hidden_dim,\n",
        "                           dropout = dropout, \n",
        "                           use_crf=True)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwcwMuZML7Oq",
        "outputId": "82a16581-e389-455d-dba3-86f18c59ca5e"
      },
      "source": [
        "%%time\n",
        "# This cell takes about 9 hours to run\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for model in models:\n",
        "  print(f'Model configuration: {model}')\n",
        "  accuracy = train_and_evaluate(model, epochs)\n",
        "  print(f'Accuracy on test dataset: {accuracy}')\n",
        "  print(\"\")\n",
        "  print(\"\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model configuration: BERT: False, LSTM: True, CRF: False\n",
            "Epoch 10: early stop\n",
            "Accuracy on validation dataset: 0.8930275746961611\n",
            "Training took: 0:05:35\n",
            "Accuracy on test dataset: 0.8961605954111564\n",
            "\n",
            "\n",
            "Model configuration: BERT: False, LSTM: True, CRF: True\n",
            "Epoch 5: early stop\n",
            "Accuracy on validation dataset: 0.8921342767813065\n",
            "Training took: 0:22:32\n",
            "Accuracy on test dataset: 0.8936436471756077\n",
            "\n",
            "\n",
            "Model configuration: BERT: True, LSTM: False, CRF: False\n",
            "Epoch 5: early stop\n",
            "Accuracy on validation dataset: 0.8976703730694187\n",
            "Training took: 1:20:09\n",
            "Accuracy on test dataset: 0.901307401709643\n",
            "\n",
            "\n",
            "Model configuration: BERT: True, LSTM: True, CRF: False\n",
            "Freezing BERT layers\n",
            "Epoch 19: early stop\n",
            "Accuracy on validation dataset: 0.8791697030959825\n",
            "Training took: 1:11:53\n",
            "Unfreezing BERT for fine tuning\n",
            "Epoch 4: early stop\n",
            "Accuracy on validation dataset: 0.8962363948376784\n",
            "Training took: 1:10:48\n",
            "Accuracy on test dataset: 0.8987151802558349\n",
            "\n",
            "\n",
            "Model configuration: BERT: True, LSTM: False, CRF: True\n",
            "Freezing BERT layers\n",
            "Epoch 7: early stop\n",
            "Accuracy on validation dataset: 0.3723641834551823\n",
            "Training took: 0:46:20\n",
            "Unfreezing BERT for fine tuning\n",
            "Epoch 1: early stop\n",
            "Accuracy on validation dataset: 0.8945673382072922\n",
            "Training took: 0:53:22\n",
            "Accuracy on test dataset: 0.8963487784568049\n",
            "\n",
            "\n",
            "Model configuration: BERT: True, LSTM: True, CRF: True\n",
            "Freezing BERT layers\n",
            "Epoch 19: early stop\n",
            "Accuracy on validation dataset: 0.8791461952561179\n",
            "Training took: 1:55:25\n",
            "Unfreezing BERT for fine tuning\n",
            "Epoch 5: early stop\n",
            "Accuracy on validation dataset: 0.8968593525940901\n",
            "Training took: 1:37:41\n",
            "Accuracy on test dataset: 0.8977413329946039\n",
            "\n",
            "\n",
            "CPU times: user 7h 36min 5s, sys: 1h 50min 53s, total: 9h 26min 58s\n",
            "Wall time: 9h 27min 26s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JW6a5XrTOKb"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kusJ7snSBjYE"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}